{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features are based in the difference between the number of some classes of tokens between the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance of numbers of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "def tokenize(row):\n",
    "    \n",
    "    return (tokenizer.tokenize(row['question1']), tokenizer.tokenize(row['question2']))\n",
    "\n",
    "def remove_stops(tokens):\n",
    "    \n",
    "    non_stop_tokens_0 = [token for token in tokens[0] if token not in stops]\n",
    "    non_stop_tokens_1 = [token for token in tokens[1] if token not in stops]\n",
    "    \n",
    "    return (non_stop_tokens_0, non_stop_tokens_1)\n",
    "\n",
    "def n_tokens_non_stop(tokens):\n",
    "    \n",
    "    return (len(remove_stops(tokens[0])), len(remove_stopstokens[1]))\n",
    "\n",
    "def n_common_tokens(tokens):\n",
    "    \n",
    "    return len(set(tokens[0]).intersection(tokens[1]))\n",
    "\n",
    "def distance(tokens):\n",
    "    \n",
    "    return abs(len(tokens[0]) - len(tokens[1]))\n",
    "\n",
    "def analyze_synsets(tokens):\n",
    "    \n",
    "    all_synsets_0 = set().union(*[wordnet.synsets(token) for token in tokens[0]])\n",
    "    all_synsets_1 = set().union(*[wordnet.synsets(token) for token in tokens[1]])\n",
    "    \n",
    "    return (all_synsets_0, all_synsets_1)\n",
    "\n",
    "def n_common_synsets(synsets):\n",
    "    \n",
    "    return len(set(synsets[0]).intersection(synsets[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(row):\n",
    "    \n",
    "    tokens = tokenize(row)\n",
    "    non_stop_tokens = remove_stops(tokens)\n",
    "    synsets = analyze_synsets(non_stop_tokens)\n",
    "    \n",
    "    distance_all_tokens = distance(tokens)\n",
    "    distance_non_stops_tokens = distance(non_stop_tokens)\n",
    "    distance_common_tokens = n_common_tokens(tokens)\n",
    "    distance_common_non_stops_tokens = n_common_tokens(non_stop_tokens)\n",
    "    distance_common_synsets = n_common_synsets(synsets)\n",
    "    \n",
    "    features = {\n",
    "        'distance_all_tokens': distance_all_tokens,\n",
    "        'distance_non_stops_tokens': distance_non_stops_tokens,\n",
    "        'distance_common_tokens': distance_common_tokens,\n",
    "        'distance_common_non_stops_tokens': distance_common_non_stops_tokens,\n",
    "        'distance_common_synsets': distance_common_synsets,\n",
    "        'id': row.name\n",
    "    }\n",
    "    \n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"../data/train.csv/train.csv\", index_col='id', dtype={'is_duplicate': 'bool'})\n",
    "\n",
    "train = train.fillna(' ')\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = train.apply(make_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = features, train['is_duplicate'].astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.559718097651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([('rfc', RandomForestClassifier(random_state=1))])\n",
    "\n",
    "\n",
    "param_grid = [{'rfc__max_depth': [1, 5],\n",
    "               'rfc__n_estimators': [10, 20]}]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='neg_log_loss',\n",
    "                  cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rfc__n_estimators': 10, 'rfc__max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('rfc', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=1,\n",
       "            verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gs.best_estimator_\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    233210\n",
       "1    171080\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model.predict(X)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running with test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(\"../data/test.csv/test.csv\", index_col='test_id', dtype={'is_duplicate': 'bool'})\n",
    "\n",
    "test = test.fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features = test.apply(make_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1736094\n",
       "1     609702\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y.ravel()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'test_id': test.index, 'is_duplicate': y.ravel()}).to_csv(\"../submission/all.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
